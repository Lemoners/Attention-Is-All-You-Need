{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Prcocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This script handles the training process.\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import dill as pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.data import Field, Dataset, BucketIterator\n",
    "from torchtext.legacy.datasets import TranslationDataset\n",
    "\n",
    "import transformer.Constants as Constants\n",
    "from transformer.Models import Transformer\n",
    "from transformer.Optim import ScheduledOptim\n",
    "\n",
    "__author__ = \"Yu-Hsiang Huang\"\n",
    "\n",
    "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "\n",
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0, 1)\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg = trg.transpose(0, 1)\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold\n",
    "\n",
    "\n",
    "def train_epoch(model, training_data, optimizer, opt, device, smoothing):\n",
    "    ''' Epoch operation in training phase'''\n",
    "\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for batch in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "        # prepare data\n",
    "        src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "        trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, opt.trg_pad_idx, smoothing=smoothing) \n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "def eval_epoch(model, validation_data, device, opt):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "\n",
    "    desc = '  - (Validation) '\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "            # prepare data\n",
    "            src_seq = patch_src(batch.src, opt.src_pad_idx).to(device)\n",
    "            trg_seq, gold = map(lambda x: x.to(device), patch_trg(batch.trg, opt.trg_pad_idx))\n",
    "\n",
    "            # forward\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, opt.trg_pad_idx, smoothing=False)\n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "\n",
    "def train(model, training_data, validation_data, optimizer, device, opt):\n",
    "    ''' Start training '''\n",
    "\n",
    "    # Use tensorboard to plot curves, e.g. perplexity, accuracy, learning rate\n",
    "    if opt.use_tb:\n",
    "        print(\"[Info] Use Tensorboard\")\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        tb_writer = SummaryWriter(log_dir=os.path.join(opt.output_dir, 'tensorboard'))\n",
    "\n",
    "    log_train_file = os.path.join(opt.output_dir, 'train.log')\n",
    "    log_valid_file = os.path.join(opt.output_dir, 'valid.log')\n",
    "\n",
    "    print('[Info] Training performance will be written to file: {} and {}'.format(\n",
    "        log_train_file, log_valid_file))\n",
    "\n",
    "    with open(log_train_file, 'w') as log_tf, open(log_valid_file, 'w') as log_vf:\n",
    "        log_tf.write('epoch,loss,ppl,accuracy\\n')\n",
    "        log_vf.write('epoch,loss,ppl,accuracy\\n')\n",
    "\n",
    "    def print_performances(header, ppl, accu, start_time, lr):\n",
    "        print('  - {header:12} ppl: {ppl: 8.5f}, accuracy: {accu:3.3f} %, lr: {lr:8.5f}, '\\\n",
    "              'elapse: {elapse:3.3f} min'.format(\n",
    "                  header=f\"({header})\", ppl=ppl,\n",
    "                  accu=100*accu, elapse=(time.time()-start_time)/60, lr=lr))\n",
    "\n",
    "    #valid_accus = []\n",
    "    valid_losses = []\n",
    "    for epoch_i in range(opt.epoch):\n",
    "        print('[ Epoch', epoch_i, ']')\n",
    "\n",
    "        start = time.time()\n",
    "        train_loss, train_accu = train_epoch(\n",
    "            model, training_data, optimizer, opt, device, smoothing=opt.label_smoothing)\n",
    "        train_ppl = math.exp(min(train_loss, 100))\n",
    "        # Current learning rate\n",
    "        lr = optimizer._optimizer.param_groups[0]['lr']\n",
    "        print_performances('Training', train_ppl, train_accu, start, lr)\n",
    "\n",
    "        start = time.time()\n",
    "        valid_loss, valid_accu = eval_epoch(model, validation_data, device, opt)\n",
    "        valid_ppl = math.exp(min(valid_loss, 100))\n",
    "        print_performances('Validation', valid_ppl, valid_accu, start, lr)\n",
    "\n",
    "        valid_losses += [valid_loss]\n",
    "\n",
    "        checkpoint = {'epoch': epoch_i, 'settings': opt, 'model': model.state_dict()}\n",
    "\n",
    "        if opt.save_mode == 'all':\n",
    "            model_name = 'model_accu_{accu:3.3f}.chkpt'.format(accu=100*valid_accu)\n",
    "            torch.save(checkpoint, model_name)\n",
    "        elif opt.save_mode == 'best':\n",
    "            model_name = 'model.chkpt'\n",
    "            if valid_loss <= min(valid_losses):\n",
    "                torch.save(checkpoint, os.path.join(opt.output_dir, model_name))\n",
    "                print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "        with open(log_train_file, 'a') as log_tf, open(log_valid_file, 'a') as log_vf:\n",
    "            log_tf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                epoch=epoch_i, loss=train_loss,\n",
    "                ppl=train_ppl, accu=100*train_accu))\n",
    "            log_vf.write('{epoch},{loss: 8.5f},{ppl: 8.5f},{accu:3.3f}\\n'.format(\n",
    "                epoch=epoch_i, loss=valid_loss,\n",
    "                ppl=valid_ppl, accu=100*valid_accu))\n",
    "\n",
    "        if opt.use_tb:\n",
    "            tb_writer.add_scalars('ppl', {'train': train_ppl, 'val': valid_ppl}, epoch_i)\n",
    "            tb_writer.add_scalars('accuracy', {'train': train_accu*100, 'val': valid_accu*100}, epoch_i)\n",
    "            tb_writer.add_scalar('learning_rate', lr, epoch_i)\n",
    "\n",
    "def prepare_dataloaders_from_bpe_files(opt, device):\n",
    "    batch_size = opt.batch_size\n",
    "    MIN_FREQ = 2\n",
    "    if not opt.embs_share_weight:\n",
    "        raise\n",
    "\n",
    "    data = pickle.load(open(opt.data_pkl, 'rb'))\n",
    "    MAX_LEN = data['settings'].max_len\n",
    "    field = data['vocab']\n",
    "    fields = (field, field)\n",
    "\n",
    "    def filter_examples_with_length(x):\n",
    "        return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "\n",
    "    train = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=opt.train_path, \n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "    val = TranslationDataset(\n",
    "        fields=fields,\n",
    "        path=opt.val_path, \n",
    "        exts=('.src', '.trg'),\n",
    "        filter_pred=filter_examples_with_length)\n",
    "\n",
    "    opt.max_token_seq_len = MAX_LEN + 2\n",
    "    opt.src_pad_idx = opt.trg_pad_idx = field.vocab.stoi[Constants.PAD_WORD]\n",
    "    opt.src_vocab_size = opt.trg_vocab_size = len(field.vocab)\n",
    "\n",
    "    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n",
    "    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n",
    "    return train_iterator, val_iterator\n",
    "\n",
    "\n",
    "def prepare_dataloaders(opt, device):\n",
    "    batch_size = opt.batch_size\n",
    "    data = pickle.load(open(opt.data_pkl, 'rb'))\n",
    "\n",
    "    opt.max_token_seq_len = data['settings'].max_len\n",
    "    opt.src_pad_idx = data['vocab']['src'].vocab.stoi[Constants.PAD_WORD]\n",
    "    opt.trg_pad_idx = data['vocab']['trg'].vocab.stoi[Constants.PAD_WORD]\n",
    "\n",
    "    opt.src_vocab_size = len(data['vocab']['src'].vocab)\n",
    "    opt.trg_vocab_size = len(data['vocab']['trg'].vocab)\n",
    "\n",
    "    #========= Preparing Model =========#\n",
    "    if opt.embs_share_weight:\n",
    "        assert data['vocab']['src'].vocab.stoi == data['vocab']['trg'].vocab.stoi, \\\n",
    "            'To sharing word embedding the src/trg word2idx table shall be the same.'\n",
    "\n",
    "    fields = {'src': data['vocab']['src'], 'trg':data['vocab']['trg']}\n",
    "\n",
    "    train = Dataset(examples=data['train'], fields=fields)\n",
    "    val = Dataset(examples=data['valid'], fields=fields)\n",
    "\n",
    "    train_iterator = BucketIterator(train, batch_size=batch_size, device=device, train=True)\n",
    "    val_iterator = BucketIterator(val, batch_size=batch_size, device=device)\n",
    "\n",
    "    return train_iterator, val_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'trg_pad_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15531/2165754292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_word_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'trg_pad_idx'"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "''' \n",
    "Usage:\n",
    "python train.py -data_pkl m30k_deen_shr.pkl -log m30k_deen_shr -embs_share_weight -proj_share_weight -label_smoothing -output_dir output -b 256 -warmup 128000\n",
    "'''\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-data_pkl', default=None)     # all-in-1 data pickle or bpe field\n",
    "\n",
    "parser.add_argument('-train_path', default=None)   # bpe encoded data\n",
    "parser.add_argument('-val_path', default=None)     # bpe encoded data\n",
    "\n",
    "parser.add_argument('-epoch', type=int, default=10)\n",
    "parser.add_argument('-b', '--batch_size', type=int, default=2048)\n",
    "\n",
    "parser.add_argument('-d_model', type=int, default=512)\n",
    "parser.add_argument('-d_inner_hid', type=int, default=2048)\n",
    "parser.add_argument('-d_k', type=int, default=64)\n",
    "parser.add_argument('-d_v', type=int, default=64)\n",
    "\n",
    "parser.add_argument('-n_head', type=int, default=8)\n",
    "parser.add_argument('-n_layers', type=int, default=6)\n",
    "parser.add_argument('-warmup','--n_warmup_steps', type=int, default=4000)\n",
    "parser.add_argument('-lr_mul', type=float, default=2.0)\n",
    "parser.add_argument('-seed', type=int, default=None)\n",
    "\n",
    "parser.add_argument('-dropout', type=float, default=0.1)\n",
    "parser.add_argument('-embs_share_weight', action='store_true')\n",
    "parser.add_argument('-proj_share_weight', action='store_true')\n",
    "parser.add_argument('-scale_emb_or_prj', type=str, default='prj')\n",
    "\n",
    "parser.add_argument('-output_dir', type=str, default=None)\n",
    "parser.add_argument('-use_tb', action='store_true')\n",
    "parser.add_argument('-save_mode', type=str, choices=['all', 'best'], default='best')\n",
    "\n",
    "parser.add_argument('-no_cuda', action='store_true')\n",
    "parser.add_argument('-label_smoothing', action='store_true')\n",
    "\n",
    "opt = parser.parse_args(args=[\"-data_pkl\", \"m30k_deen_shr.pkl\", \"-embs_share_weight\", \"-proj_share_weight\", \"-label_smoothing\", \"-output_dir\", \"output\", \"-b\", \"256\", \"-warmup\", \"128000\", \"-epoch\", \"400\", \"-no_cuda\"])\n",
    "opt.cuda = not opt.no_cuda\n",
    "opt.d_word_vec = opt.d_model\n",
    "\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "# For reproducibility\n",
    "if opt.seed is not None:\n",
    "    torch.manual_seed(opt.seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch.set_deterministic(True)\n",
    "    np.random.seed(opt.seed)\n",
    "    random.seed(opt.seed)\n",
    "\n",
    "if not opt.output_dir:\n",
    "    print('No experiment result will be saved.')\n",
    "    raise\n",
    "\n",
    "if not os.path.exists(opt.output_dir):\n",
    "    os.makedirs(opt.output_dir)\n",
    "\n",
    "if opt.batch_size < 2048 and opt.n_warmup_steps <= 4000:\n",
    "    print('[Warning] The warmup steps may be not enough.\\n'\\\n",
    "            '(sz_b, warmup) = (2048, 4000) is the official setting.\\n'\\\n",
    "            'Using smaller batch w/o longer warmup may cause '\\\n",
    "            'the warmup stage ends with only little data trained.')\n",
    "\n",
    "device = torch.device('cuda' if opt.cuda else 'cpu')\n",
    "\n",
    "#========= Loading Dataset =========#\n",
    "\n",
    "if all((opt.train_path, opt.val_path)):\n",
    "    training_data, validation_data = prepare_dataloaders_from_bpe_files(opt, device)\n",
    "elif opt.data_pkl:\n",
    "    training_data, validation_data = prepare_dataloaders(opt, device)\n",
    "else:\n",
    "    raise\n",
    "\n",
    "print(opt)\n",
    "\n",
    "transformer = Transformer(\n",
    "    opt.src_vocab_size,\n",
    "    opt.trg_vocab_size,\n",
    "    src_pad_idx=opt.src_pad_idx,\n",
    "    trg_pad_idx=opt.trg_pad_idx,\n",
    "    trg_emb_prj_weight_sharing=opt.proj_share_weight,\n",
    "    emb_src_trg_weight_sharing=opt.embs_share_weight,\n",
    "    d_k=opt.d_k,\n",
    "    d_v=opt.d_v,\n",
    "    d_model=opt.d_model,\n",
    "    d_word_vec=opt.d_word_vec,\n",
    "    d_inner=opt.d_inner_hid,\n",
    "    n_layers=opt.n_layers,\n",
    "    n_head=opt.n_head,\n",
    "    dropout=opt.dropout,\n",
    "    scale_emb_or_prj=opt.scale_emb_or_prj).to(device)\n",
    "\n",
    "optimizer = ScheduledOptim(\n",
    "    optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "    opt.lr_mul, opt.d_model, opt.n_warmup_steps)\n",
    "\n",
    "train(transformer, training_data, validation_data, optimizer, device, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8477395a26597b2fad409a1b44d0c3cd5a51da12cfe30810a128d393d6d57e45"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('decision-transformer-atari': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
